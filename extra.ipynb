{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d95846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ce321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore different layer annotation attributes\n",
    "print(\"=== Exploring ev.layers ===\")\n",
    "print(f\"Type: {type(ev.layers)}\")\n",
    "if hasattr(ev.layers, 'keys'):\n",
    "    print(f\"Keys: {list(ev.layers.keys())}\")\n",
    "print(f\"Content: {ev.layers}\")\n",
    "print(f\"ILM layer data: {ev.layers['ILM'].data}\")\n",
    "print()\n",
    "\n",
    "print(\"=== Exploring ev._layers ===\")\n",
    "print(f\"Type: {type(ev._layers)}\")\n",
    "print(f\"Length: {len(ev._layers) if hasattr(ev._layers, '__len__') else 'No length'}\")\n",
    "if hasattr(ev._layers, '__iter__') and len(ev._layers) > 0:\n",
    "    print(f\"First item type: {type(ev._layers[0])}\")\n",
    "    print(f\"First item: {ev._layers[0]}\")\n",
    "    if hasattr(ev._layers[0], '__dict__'):\n",
    "        print(f\"First item attributes: {vars(ev._layers[0])}\")\n",
    "print()\n",
    "\n",
    "# Check if there are other layer-related attributes\n",
    "print(\"=== Other potential layer attributes ===\")\n",
    "layer_attrs = [attr for attr in dir(ev) if 'layer' in attr.lower()]\n",
    "print(f\"Layer-related attributes: {layer_attrs}\")\n",
    "\n",
    "# Check annotations\n",
    "annotation_attrs = [attr for attr in dir(ev) if 'annotation' in attr.lower()]\n",
    "print(f\"Annotation-related attributes: {annotation_attrs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32b107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize any B-scan with annotations\n",
    "def visualize_bscan_with_layers(bscan_idx, figsize=(15, 8))-> None:\n",
    "    \"\"\"\n",
    "    Visualize a B-scan with retinal layer annotations\n",
    "    \n",
    "    args:\n",
    "        bscan_idx: Index of the B-scan to visualize\n",
    "        figsize: Figure size for the plot\n",
    "    returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Display the B-scan image\n",
    "    plt.imshow(bscans[bscan_idx], cmap='gray', aspect='auto')\n",
    "    \n",
    "    selected_layers = [\"ILM\", \"BM\", \"ELM\", \"PR1\"]\n",
    "    colors = ['red', 'blue', 'green', 'orange']\n",
    "    #layer_names = list(layers.keys())\n",
    "    '''\n",
    "    for i, layer_name in enumerate(selected_layers):\n",
    "        layer_data = layers[layer_name].data[bscan_idx]\n",
    "        \n",
    "        valid_indices = ~np.isnan(layer_data) # Only plot non- NaN values\n",
    "        if np.any(valid_indices):\n",
    "            x_coords = np.where(valid_indices)[0]\n",
    "            y_coords = layer_data[valid_indices]\n",
    "            plt.plot(x_coords, y_coords, color=colors[i % len(colors)], \n",
    "                    linewidth=2, label=layer_name, alpha=0.8)\n",
    "    '''\n",
    "    for i, layer_name in enumerate(selected_layers):\n",
    "        if layer_name in layers:\n",
    "            layer_data = layers[layer_name].data[bscan_idx]\n",
    "            x_coords = np.arange(layer_data.shape[0])\n",
    "            plt.plot(x_coords, layer_data, color=colors[i % len(colors)],\n",
    "                    linewidth=2, label=layer_name, alpha=0.8)\n",
    "    plt.title(f\"B-scan {bscan_idx} with Retinal Layer Annotations\")\n",
    "    plt.xlabel(\"A-scan position (pixel)\")\n",
    "    plt.ylabel(\"Depth (pixel)\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ad28d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hdf5 appending logic\n",
    "    def convert_e2e_file(self, e2e_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Convert a single .e2e file and append it to the HDF5 dataset.\n",
    "        \n",
    "        Args:\n",
    "            e2e_path (str): Path to the .e2e file\n",
    "        \"\"\"\n",
    "        # Extract data from .e2e file\n",
    "        images, layers, filename = self.extract_e2e_data(e2e_path)\n",
    "        \n",
    "        logger.info(f\"Adding data from {filename} to HDF5 file: {self.hdf5_path}\")\n",
    "        \n",
    "        with h5py.File(self.hdf5_path, 'a') as f:  # 'a' mode: create or append\n",
    "            \n",
    "            # Handle images dataset\n",
    "            if 'images' not in f:\n",
    "                # Create new dataset\n",
    "                f.create_dataset('images', data=images, \n",
    "                               maxshape=(None, *images.shape[1:]),\n",
    "                               compression='gzip', compression_opts=9)\n",
    "                logger.info(f\"Created new images dataset with shape: {images.shape}\")\n",
    "            else:\n",
    "                # Append to existing dataset\n",
    "                existing_images = f['images']\n",
    "                old_size = existing_images.shape[0]\n",
    "                new_size = old_size + images.shape[0]\n",
    "                existing_images.resize((new_size, *existing_images.shape[1:]))\n",
    "                existing_images[old_size:new_size] = images\n",
    "                logger.info(f\"Appended {images.shape[0]} images. Total: {new_size}\")\n",
    "            \n",
    "            # Handle layers group\n",
    "            if 'layers' not in f:\n",
    "                layers_group = f.create_group('layers')\n",
    "            else:\n",
    "                layers_group = f['layers']\n",
    "            \n",
    "            # Add each layer\n",
    "            for layer_name, layer_data in layers.items():\n",
    "                if layer_name not in layers_group:\n",
    "                    # Create new layer dataset\n",
    "                    layers_group.create_dataset(layer_name, data=layer_data,\n",
    "                                              maxshape=(None, *layer_data.shape[1:]),\n",
    "                                              compression='gzip', compression_opts=9)\n",
    "                    logger.info(f\"Created new layer dataset '{layer_name}' with shape: {layer_data.shape}\")\n",
    "                else:\n",
    "                    # Append to existing layer dataset\n",
    "                    existing_layer = layers_group[layer_name]\n",
    "                    old_size = existing_layer.shape[0]\n",
    "                    new_size = old_size + layer_data.shape[0]\n",
    "                    existing_layer.resize((new_size, *existing_layer.shape[1:]))\n",
    "                    existing_layer[old_size:new_size] = layer_data\n",
    "                    logger.info(f\"Appended to layer '{layer_name}'. Total: {new_size}\")\n",
    "            \n",
    "            # Handle filenames dataset\n",
    "            if 'names' not in f:\n",
    "                # Create string dataset for filenames\n",
    "                dt = h5py.string_dtype(encoding='utf-8')\n",
    "                filenames_array = np.array([filename] * images.shape[0], dtype=dt)\n",
    "                f.create_dataset('names', data=filenames_array,\n",
    "                               maxshape=(None,), compression='gzip', compression_opts=9)\n",
    "                logger.info(f\"Created new names dataset\")\n",
    "            else:\n",
    "                # Append filenames\n",
    "                existing_names = f['names']\n",
    "                old_size = existing_names.shape[0]\n",
    "                new_size = old_size + images.shape[0]\n",
    "                dt = h5py.string_dtype(encoding='utf-8')\n",
    "                filenames_array = np.array([filename] * images.shape[0], dtype=dt)\n",
    "                existing_names.resize((new_size,))\n",
    "                existing_names[old_size:new_size] = filenames_array\n",
    "                logger.info(f\"Appended filenames. Total: {new_size}\")\n",
    "        \n",
    "        logger.info(f\"Successfully processed: {e2e_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37afdcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
